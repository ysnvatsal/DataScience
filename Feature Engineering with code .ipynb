{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. _IMPUTATION_\n",
    "\n",
    "The most simple solution to the missing values is to drop the rows or the entire column. There is not an optimum \n",
    "threshold for dropping but you can use 70% as an example value and try to drop the rows and columns which have missing \n",
    "values with higher than this threshold. This is as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.7\n",
    "#Dropping columns with missing value rate higher than threshold\n",
    "data = data[data.columns[data.isnull().mean() < threshold]] \n",
    "\n",
    "#Dropping rows with missing value rate higher than threshold\n",
    "data = data.loc[data.isnull().mean(axis=1) < threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) _NUMERICAL IMPUTATION_ : Numerical imputation is done based on the nature of the feature. We can generally replace with 0 or median or mean. We can use the following code as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling all missing values with 0\n",
    "data = data.fillna(0)\n",
    "#Filling missing values with medians of the columns\n",
    "data = data.fillna(data.median())\n",
    "data = data.fillna(data.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) _Categorical Imputation_ : Replacing the missing values with the maximum occurred value in a column is a good option for handling categorical columns. We can use the following code as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Max fill function for categorical columns\n",
    "data['column_name'].fillna(data['column_name'].value_counts().idxmax(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. _HANDLING OUTLIERS_\n",
    "Outliers are handed using two ways : 1) Standard deviation\n",
    "                                     2) Percentiles\n",
    "        If a value has a distance to the average higher than x * standard deviation, it can be assumed as an outlier.\n",
    "        X here is usuallynot trivial. It can be taken any value between 2 and 4\n",
    "        \n",
    "        Another option for handling outliers is to cap them instead of dropping. So you can keep your data size and at the end of the day, it might be better for the final model performance.On the other hand, capping can affect the distribution of the data, thus it better not to exaggerate it.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the outlier rows with standard deviation\n",
    "factor = 3\n",
    "upper_lim = data['column'].mean () + data['column'].std () * factor\n",
    "lower_lim = data['column'].mean () - data['column'].std () * factor\n",
    "\n",
    "data = data[(data['column'] < upper_lim) & (data['column'] > lower_lim)]\n",
    "\n",
    "#Dropping the outlier rows with Percentiles\n",
    "upper_lim = data['column'].quantile(.95)\n",
    "lower_lim = data['column'].quantile(.05)\n",
    "\n",
    "data = data[(data['column'] < upper_lim) & (data['column'] > lower_lim)]\n",
    "\n",
    "#Capping the outlier rows with Percentiles\n",
    "upper_lim = data['column'].quantile(.95)\n",
    "lower_lim = data['column'].quantile(.05)\n",
    "data.loc[(df[column] > upper_lim),column] = upper_lim\n",
    "data.loc[(df[column] < lower_lim),column] = lower_lim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) _Binning_: \n",
    "    \n",
    "    The main motivation of binning is to make the model more robust and prevent overfitting, however, it has a cost to\n",
    "    the performance. Every time you bin something, you sacrifice information and make your data more regularized. \n",
    "    The trade-off between performance and overfitting is the key point of the binning process. In my opinion, \n",
    "    for numerical columns, except for some obvious overfitting cases, binning might be redundant for some kind of \n",
    "    algorithms, due to its effect on model performance. However, for categorical columns, the labels with low frequencies\n",
    "    probably affect the robustness of statistical models negatively. Thus, assigning a general category to these \n",
    "    less frequent values helps to keep the robustness of the model. For example, if your data size is 100,000 rows, \n",
    "    it might be a good option to unite the labels with a count less than 100 to a new category like “Other”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Eg:\n",
    "        Country\n",
    "0      Spain\n",
    "1      Chile\n",
    "2  Australia\n",
    "3      Italy\n",
    "4     Brazil\n",
    "conditions = [\n",
    "    data['Country'].str.contains('Spain'),\n",
    "    data['Country'].str.contains('Italy'),\n",
    "    data['Country'].str.contains('Chile'),\n",
    "    data['Country'].str.contains('Brazil')]\n",
    "\n",
    "choices = ['Europe', 'Europe', 'South America', 'South America']\n",
    "\n",
    "data['Continent'] = np.select(conditions, choices, default='Other')\n",
    "     Country      Continent\n",
    "0      Spain         Europe\n",
    "1      Chile  South America\n",
    "2  Australia          Other\n",
    "3      Italy         Europe\n",
    "4     Brazil  South America"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4) Transformation:\n",
    "    \n",
    "    Eg Log transformation\n",
    "    data['log+1'] = (data['value']+1).transform(np.log)\n",
    "    \n",
    "    #Negative Values Handling\n",
    "#Note that the values are different\n",
    "data['log'] = (data['value']-data['value'].min()+1) .transform(np.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5) One hot encoding :\n",
    "    Will be in the part of data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6) Scaling:\n",
    "    a. Standardisation\n",
    "    \n",
    "    \n",
    "data_scaled = StandardScaler()\n",
    "data_scaled.fit(project_data['column'].values.reshape(-1,1)) # finding the mean and standard deviation of this data\n",
    "print(f\"Mean : {data_scaled.mean_[0]}, Standard deviation : {np.sqrt(data_scaled.var_[0])}\")\n",
    "\n",
    "# Now standardize the data with above maen and variance.\n",
    "column_values = data_scaled.transform(data['column'].values.reshape(-1, 1))\n",
    "\n",
    "    b. Normalization\n",
    "# Normalize total_bedrooms column\n",
    "x_data = np.array(data['total_bedrooms'])\n",
    "normalized_data = preprocessing.normalize([x_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "7) Extracting date:\n",
    "    \n",
    "#Transform string to date\n",
    "data['date'] = pd.to_datetime(data.date, format=\"%d-%m-%Y\")\n",
    "\n",
    "#Extracting Year\n",
    "data['year'] = data['date'].dt.year\n",
    "\n",
    "#Extracting Month\n",
    "data['month'] = data['date'].dt.month\n",
    "\n",
    "#Extracting passed years since the date\n",
    "data['passed_years'] = date.today().year - data['date'].dt.year\n",
    "\n",
    "#Extracting passed months since the date\n",
    "data['passed_months'] = (date.today().year - data['date'].dt.year) * 12 + date.today().month - data['date'].dt.month\n",
    "\n",
    "#Extracting the weekday name of the date\n",
    "data['day_name'] = data['date'].dt.day_name()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
